name: Convert HF model to GGUF for Ollama

on:
  workflow_dispatch:            # lancement manuel depuis l’onglet Actions

jobs:
  convert-and-publish:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣ Récupère ton dépôt GitHub
      - name: Checkout repo
        uses: actions/checkout@v4

      # 2️⃣ Installe Git-LFS + Python + dépendances de conversion
      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y git-lfs python3-pip
          pip install --upgrade pip
          pip install transformers sentencepiece

      # 3️⃣ Clone uniquement le code de llama.cpp (pas besoin de compiler)
      - name: Clone llama.cpp (converter only)
        run: |
          git clone --depth 1 https://github.com/ggerganov/llama.cpp

      # 4️⃣ (Option debug) affiche la longueur du token pour vérifier qu’il arrive
      - name: Debug HF_TOKEN length
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: echo "HF_TOKEN length = ${#HF_TOKEN}"

      # 5️⃣ Télécharge le modèle Hugging Face avec le token
      - name: Download HF model
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          git lfs install
          # username vide, token en mot de passe → syntaxe officielle HF
          git clone https://:${HF_TOKEN}@huggingface.co/BioMistral/BioMistral-7B-DARE
          cd BioMistral-7B-DARE && git lfs pull

      # 6️⃣ Convertit vers GGUF (script Python)
      - name: Convert to GGUF
        run: |
          python3 llama.cpp/convert.py BioMistral-7B-DARE \
                 --outfile biomistral7b-dare.gguf

      # 7️⃣ Uploade le fichier GGUF en artefact
      - name: Upload GGUF
        uses: actions/upload-artifact@v4
        with:
          name: biomistral7b-dare-gguf
          path: biomistral7b-dare.gguf
          if-no-files-found: error
